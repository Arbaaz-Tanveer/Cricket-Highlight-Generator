{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arbaaz-Tanveer/Cricket-Highlight-Generator/blob/main/generator_with_yolo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzCBRi5yI0Xt"
      },
      "outputs": [],
      "source": [
        "# Cricket Highlights Generator - Combined Audio and OCR Approach with Scene Change Adjustment and YOLO Scoreboard Cropping\n",
        "\n",
        "# Install dependencies\n",
        "!pip install pydub opencv-python-headless matplotlib numpy tqdm paddleocr paddlepaddle ultralytics -q\n",
        "!apt-get install ffmpeg -qq\n",
        "\n",
        "import cv2\n",
        "import os\n",
        "import glob\n",
        "import re\n",
        "import logging\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pydub import AudioSegment\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "from google.colab import files\n",
        "from paddleocr import PaddleOCR\n",
        "from ultralytics import YOLO  # Import YOLO from ultralytics\n",
        "\n",
        "# -------------------- PATH SETUP --------------------\n",
        "content_dir = '/content'\n",
        "output_dir = os.path.join(content_dir, 'highlights')\n",
        "temp_dir = os.path.join(content_dir, 'temp')\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "os.makedirs(temp_dir, exist_ok=True)\n",
        "\n",
        "# Clear previous output\n",
        "for f in glob.glob(os.path.join(output_dir, \"*\")):\n",
        "    os.remove(f)\n",
        "for f in glob.glob(os.path.join(temp_dir, \"*\")):\n",
        "    os.remove(f)\n",
        "\n",
        "# Logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger('CricketHighlights')\n",
        "\n",
        "# Path to uploaded video\n",
        "video_path = '/content/clip.mp4'\n",
        "if not os.path.exists(video_path):\n",
        "    print(f\"❌ Error: {video_path} not found!\")\n",
        "else:\n",
        "    print(f\"✅ Found video: {video_path}\")\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if cap.isOpened():\n",
        "        w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "        fc = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        dur = fc / fps\n",
        "        print(f\"Dimensions: {w}x{h}, Duration: {dur:.1f}s, FPS: {fps}\")\n",
        "        cap.release()\n",
        "\n",
        "# -------------------- OCR FUNCTIONS --------------------\n",
        "def clean_ocr_text(text):\n",
        "    \"\"\"Strip off parentheses and non‑score characters.\"\"\"\n",
        "    text = re.sub(r'\\(.*$', '', text)        # drop from \"(\" onward\n",
        "    text = re.sub(r'[^0-9A-Z\\-\\/\\s]', '', text) # keep digits, uppercase letters, dash, slash and spaces\n",
        "    return text\n",
        "\n",
        "def preprocess_for_ocr(cropped):\n",
        "    # Check current dimensions\n",
        "    h, w = cropped.shape[:2]\n",
        "\n",
        "    # Determine the scaling factor so that the smallest side becomes at least 500 pixels.\n",
        "    min_side = min(h, w)\n",
        "    if min_side < 500:\n",
        "        scale = 500 / min_side\n",
        "        new_w = int(w * scale)\n",
        "        new_h = int(h * scale)\n",
        "        cropped = cv2.resize(cropped, (new_w, new_h), interpolation=cv2.INTER_LINEAR)\n",
        "    print(scale)\n",
        "    # Convert to grayscale\n",
        "    gray = cv2.cvtColor(cropped, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Apply Otsu's thresholding to binarize the image\n",
        "    _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "\n",
        "    # Alternatively, you can try adaptive thresholding if needed:\n",
        "    # thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "    #                                cv2.THRESH_BINARY, 11, 2)\n",
        "\n",
        "    return thresh\n",
        "\n",
        "\n",
        "def extract_score(text):\n",
        "    \"\"\"\n",
        "    Find runs-wickets (or runs/wickets) in cleaned text.\n",
        "    Returns a string like \"10-1\" or None if not found.\n",
        "    \"\"\"\n",
        "    txt = clean_ocr_text(text)\n",
        "    m = re.search(r'(\\d{1,3})\\s*[-\\/]\\s*(\\d{1,2})', txt)\n",
        "    if not m:\n",
        "        return None\n",
        "    runs, wkts = int(m.group(1)), int(m.group(2))\n",
        "    if runs >= 0 and 0 <= wkts <= 10:\n",
        "        return f\"{runs}-{wkts}\"\n",
        "    return None\n",
        "\n",
        "def extract_team_names(text):\n",
        "    \"\"\"\n",
        "    Extract team names by checking for a \"v\" or \"vs\" pattern.\n",
        "    For example, from \"PAK v IND\" it returns \"PAK v IND\".\n",
        "    \"\"\"\n",
        "    m = re.search(r'([A-Z]{2,3})\\s*(?:v|vs)\\s*([A-Z]{2,3})', text)\n",
        "    if m:\n",
        "        return f\"{m.group(1)} v {m.group(2)}\"\n",
        "    teams = ['IND','PAK','AUS','ENG','NZ','SA','WI','SL','BAN','AFG','ZIM','IRE']\n",
        "    for t in teams:\n",
        "        if t in text:\n",
        "            return t\n",
        "    return None\n",
        "\n",
        "def extract_score_and_teams(text):\n",
        "    \"\"\"\n",
        "    Attempt to extract both the team names and the score from text formatted like:\n",
        "    \"PAK v IND 10 - 1\"\n",
        "    Returns a tuple (score, team_str) or (None, None) if extraction fails.\n",
        "    \"\"\"\n",
        "    text = \" \".join(text.splitlines())\n",
        "    print(text)\n",
        "\n",
        "    txt = clean_ocr_text(text)\n",
        "    # This pattern now accepts either \"v\" or \"vs\" between team abbreviations\n",
        "    pattern = r'([A-Z]{2,3}\\s*(?:v|vs)\\s*[A-Z]{2,3}).?(\\d{1,3})\\s[-\\/]\\s*(\\d{1,2})'\n",
        "    m = re.search(pattern, txt)\n",
        "    if m:\n",
        "        teams = m.group(1).strip()\n",
        "        runs = int(m.group(2))\n",
        "        wkts = int(m.group(3))\n",
        "        if 0 <= wkts <= 10:\n",
        "            score = f\"{runs}-{wkts}\"\n",
        "            return score, teams\n",
        "    # Fallback to separate extraction if pattern matching fails\n",
        "    return extract_score(text), extract_team_names(text)\n",
        "\n",
        "# ------------- NEW: YOLO SCOREBOARD CROPPING FUNCTION -------------\n",
        "def crop_scoreboard(frame, yolo_model):\n",
        "    \"\"\"\n",
        "    Run YOLO detection on the frame to detect the scoreboard.\n",
        "    Returns the cropped region corresponding to the union of all detected bounding boxes.\n",
        "    If no detection is found, returns the original frame.\n",
        "    \"\"\"\n",
        "    results = yolo_model(frame)\n",
        "    # If no detections or no boxes, fallback to full frame\n",
        "    if not results or not results[0].boxes:\n",
        "        return frame\n",
        "\n",
        "    boxes = results[0].boxes\n",
        "    # Initialize union coordinates with extreme values\n",
        "    x_min, y_min = float('inf'), float('inf')\n",
        "    x_max, y_max = -float('inf'), -float('inf')\n",
        "\n",
        "    # Compute union of all detected boxes.\n",
        "    # Each box.xyxy[0] returns coordinates in [x1, y1, x2, y2] format.\n",
        "    for box in boxes:\n",
        "        x1, y1, x2, y2 = box.xyxy[0].tolist()\n",
        "        x_min = min(x_min, x1)\n",
        "        y_min = min(y_min, y1)\n",
        "        x_max = max(x_max, x2)\n",
        "        y_max = max(y_max, y2)\n",
        "\n",
        "    # Convert coordinates to integer values\n",
        "    x_min, y_min, x_max, y_max = int(x_min), int(y_min), int(x_max), int(y_max)\n",
        "    # Crop the union region from the frame\n",
        "    cropped = frame[y_min:y_max, x_min:x_max]\n",
        "    return cropped\n",
        "\n",
        "\n",
        "def detect_scoreboard(frame, ocr_model, yolo_model):\n",
        "    # Crop the scoreboard using YOLO\n",
        "    cropped = crop_scoreboard(frame, yolo_model)\n",
        "\n",
        "    # Optional: Display for debugging\n",
        "    # plt.figure(figsize=(6,4))\n",
        "    # plt.imshow(cv2.cvtColor(cropped, cv2.COLOR_BGR2RGB))\n",
        "    # plt.title(\"Cropped Scoreboard Region\")\n",
        "    # plt.axis(\"off\")\n",
        "    # plt.show()\n",
        "\n",
        "    # Preprocess the cropped image to enhance OCR results\n",
        "    preprocessed = preprocess_for_ocr(cropped)\n",
        "\n",
        "    # Optional: display the preprocessed image\n",
        "    # plt.figure(figsize=(6,4))\n",
        "    # plt.imshow(preprocessed, cmap='gray')\n",
        "    # plt.title(\"Preprocessed Scoreboard Region\")\n",
        "    # plt.axis(\"off\")\n",
        "    # plt.show()\n",
        "\n",
        "    results = ocr_model.ocr(preprocessed, cls=True)\n",
        "    if not results or results[0] is None:\n",
        "        return None, None, 0\n",
        "\n",
        "    best_score, best_team = None, None\n",
        "    best_conf = 0\n",
        "    for line in results:\n",
        "        if not line:\n",
        "            continue\n",
        "        for word_info in line:\n",
        "            if not word_info or len(word_info) < 2:\n",
        "                continue\n",
        "            txt, conf = word_info[1]\n",
        "            print(txt)\n",
        "            if 'v' in txt.lower():\n",
        "                score, teams = extract_score_and_teams(txt)\n",
        "                if score and teams and conf > best_conf:\n",
        "                    best_score, best_team, best_conf = score, teams, conf\n",
        "            else:\n",
        "                score = extract_score(txt)\n",
        "                teams = extract_team_names(txt)\n",
        "                if score and teams and conf > best_conf:\n",
        "                    best_score, best_team, best_conf = score, teams, conf\n",
        "\n",
        "    if best_score:\n",
        "        print(f\"OCR detected score {best_score}, team(s) {best_team}\")\n",
        "    return best_score, best_team, best_conf\n",
        "\n",
        "\n",
        "def analyze_video_for_scores(video_path, ocr_model, yolo_model, interval=5):\n",
        "    print(\"Analyzing video for score changes…\")\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(\"❌ Cannot open video for OCR\")\n",
        "        return []\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    duration = total / fps\n",
        "    step = int(fps * interval)\n",
        "\n",
        "    last_score, last_runs, last_wkts = None, 0, 0\n",
        "    events = []\n",
        "    pbar = tqdm(total=int(duration / interval), desc=\"Score OCR\")\n",
        "\n",
        "    frame = 0\n",
        "    while frame < total:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame)\n",
        "        ret, img = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        t = frame / fps\n",
        "\n",
        "        # Pass the YOLO model along with the OCR model\n",
        "        sc, tm, conf = detect_scoreboard(img, ocr_model, yolo_model)\n",
        "        if sc and conf > 0.5:\n",
        "            runs, wkts = map(int, sc.split('-'))\n",
        "            if last_score is None:\n",
        "                last_score, last_runs, last_wkts = sc, runs, wkts\n",
        "            else:\n",
        "                # Calculate differences from last captured score\n",
        "                dr = runs - last_runs\n",
        "                dw = wkts - last_wkts\n",
        "\n",
        "                # Discard impossible changes\n",
        "                if dr < 0 or dr > 7:\n",
        "                    last_score, last_runs, last_wkts = sc, runs, wkts\n",
        "                    frame += step\n",
        "                    pbar.update(1)\n",
        "                    continue\n",
        "\n",
        "                # Only consider events where run change is exactly 4 or exactly 6, or where a wicket falls (dw == 1)\n",
        "                if (dr in [4, 6]) or (dw == 1):\n",
        "                    print(f\"🏏 Valid change at {t:.1f}s: {last_score} → {sc} (+{dr} runs, +{dw} wkts)\")\n",
        "                    events.append(t)\n",
        "                    last_score, last_runs, last_wkts = sc, runs, wkts\n",
        "                elif dr > 0:\n",
        "                    # Update baseline for minor valid run changes that don't trigger an event\n",
        "                    last_score, last_runs, last_wkts = sc, runs, wkts\n",
        "\n",
        "        frame += step\n",
        "        pbar.update(1)\n",
        "\n",
        "    pbar.close()\n",
        "    cap.release()\n",
        "    print(f\"Found {len(events)} valid score events\")\n",
        "    return events\n",
        "\n",
        "# -------------------- AUDIO ANALYSIS FUNCTIONS --------------------\n",
        "def analyze_audio(video_path, threshold_db=-30, interval=0.5):\n",
        "    \"\"\"Extract excitement points from audio based on volume.\"\"\"\n",
        "    print(\"Analyzing audio to find exciting moments...\")\n",
        "    audio_file = os.path.join(temp_dir, \"audio.wav\")\n",
        "    os.system(f'ffmpeg -i \"{video_path}\" -vn -acodec pcm_s16le -ar 44100 -ac 2 \"{audio_file}\" -y -hide_banner -loglevel error')\n",
        "\n",
        "    if not os.path.exists(audio_file) or os.path.getsize(audio_file) == 0:\n",
        "        print(\"⚠ Audio extraction failed, retrying...\")\n",
        "        os.system(f'ffmpeg -i \"{video_path}\" -vn -acodec pcm_s16le \"{audio_file}\" -y -hide_banner -loglevel error')\n",
        "\n",
        "    if not os.path.exists(audio_file) or os.path.getsize(audio_file) == 0:\n",
        "        print(\"❌ Could not extract audio\")\n",
        "        return []\n",
        "\n",
        "    try:\n",
        "        audio = AudioSegment.from_file(audio_file)\n",
        "        segment_ms = int(interval * 1000)\n",
        "        volumes, timestamps = [], []\n",
        "        for i in range(0, len(audio), segment_ms):\n",
        "            seg = audio[i:i+segment_ms]\n",
        "            if len(seg) > 0:\n",
        "                volumes.append(seg.dBFS)\n",
        "                timestamps.append(i / 1000.0)\n",
        "\n",
        "        # Debug plot for audio analysis\n",
        "        plt.figure(figsize=(12, 4))\n",
        "        plt.plot(timestamps, volumes)\n",
        "        plt.axhline(y=threshold_db, color='r', linestyle='--', label=f'Threshold ({threshold_db} dB)')\n",
        "        plt.title('Audio Volume Analysis')\n",
        "        plt.xlabel('Time (s)')\n",
        "        plt.ylabel('Volume (dB)')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        excitement_points = []\n",
        "        current = None\n",
        "        min_dur = 1.0\n",
        "\n",
        "        for t, v in zip(timestamps, volumes):\n",
        "            loud = (v > threshold_db)\n",
        "            if loud and current is None:\n",
        "                current = {'start': t, 'peak_time': t, 'peak_volume': v}\n",
        "            elif loud and current:\n",
        "                if v > current['peak_volume']:\n",
        "                    current['peak_time'], current['peak_volume'] = t, v\n",
        "            elif not loud and current:\n",
        "                current['end'] = t\n",
        "                if (current['end'] - current['start']) >= min_dur:\n",
        "                    excitement_points.append(current['peak_time'])\n",
        "                current = None\n",
        "\n",
        "        if current:\n",
        "            current['end'] = timestamps[-1]\n",
        "            if (current['end'] - current['start']) >= min_dur:\n",
        "                excitement_points.append(current['peak_time'])\n",
        "\n",
        "        print(f\"Found {len(excitement_points)} exciting audio moments\")\n",
        "        if len(excitement_points) > 30:\n",
        "            top = sorted(\n",
        "                [(pt, volumes[timestamps.index(pt)]) for pt in excitement_points],\n",
        "                key=lambda x: x[1], reverse=True\n",
        "            )[:30]\n",
        "            excitement_points = sorted([pt for pt, _ in top])\n",
        "\n",
        "        return excitement_points\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error analyzing audio: {e}\")\n",
        "        import traceback; traceback.print_exc()\n",
        "        return []\n",
        "\n",
        "# -------------------- SCENE CHANGE FUNCTIONS --------------------\n",
        "def get_frame_feature(frame):\n",
        "    \"\"\"\n",
        "    Compute a normalized color histogram as the feature vector for the given frame.\n",
        "    The histogram is computed in the HSV color space for robustness.\n",
        "    \"\"\"\n",
        "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
        "    hist = cv2.calcHist([hsv], [0, 1], None, [50, 60], [0, 180, 0, 256])\n",
        "    hist = cv2.normalize(hist, hist).flatten()\n",
        "    return hist\n",
        "\n",
        "def chi_square_distance(histA, histB, eps=1e-10):\n",
        "    \"\"\"Compute the Chi-Square distance between two histograms.\"\"\"\n",
        "    return 0.5 * np.sum(((histA - histB) ** 2) / (histA + histB + eps))\n",
        "\n",
        "def merge_close_timestamps(timestamps, merge_threshold=0.2):\n",
        "    \"\"\"\n",
        "    Merge timestamps that are within merge_threshold seconds of each other.\n",
        "    Returns a list of merged timestamps (average of clustered values).\n",
        "    \"\"\"\n",
        "    if not timestamps:\n",
        "        return []\n",
        "\n",
        "    merged = []\n",
        "    cluster = [timestamps[0]]\n",
        "    for t in timestamps[1:]:\n",
        "        if t - cluster[-1] <= merge_threshold:\n",
        "            cluster.append(t)\n",
        "        else:\n",
        "            merged.append(sum(cluster) / len(cluster))\n",
        "            cluster = [t]\n",
        "    if cluster:\n",
        "        merged.append(sum(cluster) / len(cluster))\n",
        "    return merged\n",
        "\n",
        "def detect_scene_changes(video_path, threshold=0.5):\n",
        "    \"\"\"Detect scene changes using a simple histogram difference approach.\"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(\"Error opening video file for scene detection:\", video_path)\n",
        "        return []\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    prev_feature = None\n",
        "    frame_count = 0\n",
        "    detected_times = []\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        # Resize frame to reduce computation time (adjust size as needed)\n",
        "        frame = cv2.resize(frame, (320, 240))\n",
        "        current_feature = get_frame_feature(frame)\n",
        "        if prev_feature is not None:\n",
        "            diff = np.linalg.norm(current_feature - prev_feature)\n",
        "            if diff > threshold:\n",
        "                timestamp = frame_count / fps\n",
        "                detected_times.append(timestamp)\n",
        "        prev_feature = current_feature\n",
        "        frame_count += 1\n",
        "\n",
        "    cap.release()\n",
        "    merged_times = merge_close_timestamps(detected_times)\n",
        "    print(f\"Detected {len(merged_times)} scene changes at: {merged_times}\")\n",
        "    return merged_times\n",
        "\n",
        "def adjust_lower_bound(lower_time, scene_times, min_gap=3):\n",
        "    \"\"\"\n",
        "    For the desired lower_time, return the nearest scene change timestamp that has its next scene change\n",
        "    at least min_gap seconds in the future.\n",
        "    \"\"\"\n",
        "    scene_times = sorted(scene_times)\n",
        "    for i, s in enumerate(scene_times):\n",
        "        if s >= lower_time:\n",
        "            if i < len(scene_times)-1 and (scene_times[i+1] - s) >= min_gap:\n",
        "                return s\n",
        "            break\n",
        "    return lower_time\n",
        "\n",
        "def adjust_upper_bound(upper_time, scene_times, min_gap=3):\n",
        "    \"\"\"\n",
        "    For the desired upper_time, return the nearest scene change timestamp that has its previous scene change\n",
        "    at least min_gap seconds in the past.\n",
        "    \"\"\"\n",
        "    scene_times = sorted(scene_times)\n",
        "    for i in range(len(scene_times)-1, -1, -1):\n",
        "        s = scene_times[i]\n",
        "        if s <= upper_time:\n",
        "            if i > 0 and (s - scene_times[i-1]) >= min_gap:\n",
        "                return s\n",
        "            break\n",
        "    return upper_time\n",
        "\n",
        "# -------------------- CLIP EXTRACTION FUNCTIONS --------------------\n",
        "def extract_robust_clip(video_path, start_time, duration, output_file):\n",
        "    \"\"\"Extract a single clip with robust error handling.\"\"\"\n",
        "    s, d = int(start_time), int(duration)\n",
        "    cmd = f'ffmpeg -i \"{video_path}\" -ss {s} -t {d} -c:v libx264 -preset ultrafast -c:a aac \"{output_file}\" -y -hide_banner -loglevel error'\n",
        "    res = os.system(cmd)\n",
        "    if res != 0 or not os.path.exists(output_file) or os.path.getsize(output_file) < 10000:\n",
        "        cmd = f'ffmpeg -ss {s} -i \"{video_path}\" -t {d} -c:v libx264 -preset ultrafast -c:a aac \"{output_file}\" -y -hide_banner -loglevel error'\n",
        "        res = os.system(cmd)\n",
        "    if res != 0 or not os.path.exists(output_file) or os.path.getsize(output_file) < 10000:\n",
        "        cmd = f'ffmpeg -ss {s} -i \"{video_path}\" -t {d} -c copy \"{output_file}\" -y -hide_banner -loglevel error'\n",
        "        res = os.system(cmd)\n",
        "    return (res == 0 and os.path.exists(output_file) and os.path.getsize(output_file) > 10000)\n",
        "\n",
        "def extract_clips(video_path, event_timestamps, scene_times, pre_sec=15, post_sec=25):\n",
        "    \"\"\"Extract clips for each event timestamp with overlap handling and scene boundary adjustments.\"\"\"\n",
        "    if not event_timestamps:\n",
        "        print(\"❌ No event timestamps to extract clips for\")\n",
        "        return 0\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    duration = total / fps\n",
        "    cap.release()\n",
        "\n",
        "    segments = []\n",
        "    for ts in event_timestamps:\n",
        "        # Compute initial clip boundaries based on audio/OCR events\n",
        "        lower = max(0, ts - pre_sec)\n",
        "        upper = min(duration, ts + post_sec)\n",
        "        # Adjust using scene change timestamps:\n",
        "        adjusted_lower = adjust_lower_bound(lower, scene_times, min_gap=3)\n",
        "        adjusted_upper = adjust_upper_bound(upper, scene_times, min_gap=3)\n",
        "        # Use the adjusted boundaries (ensure lower is less than upper)\n",
        "        start, end = adjusted_lower, adjusted_upper\n",
        "        if start >= end:\n",
        "            start = lower\n",
        "            end = upper\n",
        "        # Print out the original and scene adjusted boundaries for debugging:\n",
        "        print(f\"Event at {ts:.1f}s: Original clip: {lower:.1f}s–{upper:.1f}s, Scene adjusted clip: {adjusted_lower:.1f}s–{adjusted_upper:.1f}s\")\n",
        "\n",
        "        # Merge overlapping segments\n",
        "        merged = False\n",
        "        for i, (s, e) in enumerate(segments):\n",
        "            if start <= e and end >= s:\n",
        "                segments[i] = (min(s, start), max(e, end))\n",
        "                merged = True\n",
        "                break\n",
        "        if not merged:\n",
        "            segments.append((start, end))\n",
        "\n",
        "    print(f\"Extracting {len(segments)} clips (after merging)\")\n",
        "    success = 0\n",
        "    for idx, (s, e) in enumerate(segments):\n",
        "        dur_seg = e - s\n",
        "        if dur_seg < 5:\n",
        "            continue\n",
        "        out = os.path.join(output_dir, f\"clip_{idx+1}_{int(s)}s.mp4\")\n",
        "        print(f\" ▶ Clip {idx+1}: {s:.1f}s–{e:.1f}s (duration: {dur_seg:.1f}s)\")\n",
        "        if extract_robust_clip(video_path, s, dur_seg, out):\n",
        "            print(\"   ✅\")\n",
        "            success += 1\n",
        "        else:\n",
        "            print(\"   ❌\")\n",
        "    print(f\"✅ Extracted {success}/{len(segments)} clips\")\n",
        "    return success\n",
        "\n",
        "def merge_clips():\n",
        "    \"\"\"Merge all extracted clips into a final highlights video.\"\"\"\n",
        "    clips = sorted(glob.glob(os.path.join(output_dir, \"clip_*.mp4\")),\n",
        "                   key=lambda x: int(re.search(r'clip_(\\d+)_', x).group(1)))\n",
        "    if not clips:\n",
        "        print(\"❌ No clips to merge\")\n",
        "        return False\n",
        "\n",
        "    list_file = os.path.join(temp_dir, \"clips_list.txt\")\n",
        "    with open(list_file, 'w') as f:\n",
        "        for c in clips:\n",
        "            f.write(f\"file '{c}'\\n\")\n",
        "\n",
        "    final = os.path.join(content_dir, \"cricket_highlights.mp4\")\n",
        "    cmd = f'ffmpeg -f concat -safe 0 -i \"{list_file}\" -c copy \"{final}\" -y -hide_banner -loglevel error'\n",
        "    res = os.system(cmd)\n",
        "    if res == 0 and os.path.exists(final):\n",
        "        print(f\"✅ Highlights video ready: {final}\")\n",
        "        files.download(final)\n",
        "        return True\n",
        "\n",
        "    print(\"❌ Merge failed\")\n",
        "    return False\n",
        "\n",
        "# -------------------- MAIN PROCESSING FUNCTION --------------------\n",
        "def process_video():\n",
        "    if not os.path.exists(video_path):\n",
        "        print(\"❌ Please upload 'clip.mp4' to your Colab environment first.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        # Initialize the YOLO model with your trained weight file.\n",
        "        # Update the weight file path if necessary.\n",
        "        yolo_model = YOLO('/content/best (3).pt')\n",
        "\n",
        "        # Step 0: Scene Change Detection\n",
        "        print(\"\\n🔍 Running scene change detection...\")\n",
        "        scene_times = detect_scene_changes(video_path, threshold=0.5)\n",
        "        if not scene_times:\n",
        "            print(\"❌ No scene changes detected, clips will use original boundaries.\")\n",
        "        else:\n",
        "            print(f\"Scene changes detected at: {scene_times}\")\n",
        "\n",
        "        # Step 1: Audio Analysis Parameters\n",
        "        print(\"\\n⚙ Enter parameters for highlight extraction:\")\n",
        "        audio_threshold = float(input(\"Audio threshold in dB (-40 to -20) [default -30]: \") or \"-30\")\n",
        "        pre_time = float(input(\"Seconds BEFORE exciting moment [default 15]: \") or \"15\")\n",
        "        post_time = float(input(\"Seconds AFTER exciting moment [default 25]: \") or \"25\")\n",
        "\n",
        "        # Step 2: Audio Analysis\n",
        "        audio_events = analyze_audio(video_path, threshold_db=audio_threshold, interval=0.5)\n",
        "\n",
        "        # Step 3: OCR Analysis\n",
        "        print(\"\\n🏏 Initializing OCR model (this may take a moment)...\")\n",
        "        ocr_model = PaddleOCR(use_angle_cls=True, lang='en', show_log=False)\n",
        "        score_events = analyze_video_for_scores(video_path, ocr_model, yolo_model, interval=5)\n",
        "\n",
        "        # Combine events from audio and OCR\n",
        "        all_events = sorted(set(audio_events + score_events))\n",
        "        print(f\"\\n📊 Combined Analysis:\")\n",
        "        print(f\"  • Audio events: {len(audio_events)}\")\n",
        "        print(f\"  • Score events: {len(score_events)}\")\n",
        "        print(f\"  • Total unique events: {len(all_events)}\")\n",
        "\n",
        "        if not all_events:\n",
        "            print(\"❌ No events detected. Try adjusting thresholds.\")\n",
        "            return\n",
        "\n",
        "        # Step 4: Extract Clips (with scene change adjustment)\n",
        "        print(\"\\n✂ Extracting highlight clips with scene change adjustments...\")\n",
        "        clip_count = extract_clips(video_path, all_events, scene_times, pre_sec=pre_time, post_sec=post_time)\n",
        "\n",
        "        # Step 5: Merge Clips\n",
        "        if clip_count > 0:\n",
        "            print(\"\\n🔄 Merging clips into final highlights video...\")\n",
        "            merge_clips()\n",
        "        else:\n",
        "            print(\"❌ No clips extracted. Try adjusting the parameters.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ An error occurred: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "if _name_ == \"_main_\":\n",
        "    process_video()"
      ]
    }
  ]
}